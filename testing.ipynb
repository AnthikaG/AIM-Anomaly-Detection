{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "import torchvision\n",
    "# import torchaudio\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "# print(torchaudio.__version__)\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from pytorch_i3d import InceptionI3d\n",
    "import videotransforms\n",
    "\n",
    "!pip3 install av\n",
    "import av\n",
    "print(av.__version__)\n",
    "\n",
    "# !pip3 install opencv-python\n",
    "!pip3 install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root_dir = 'short videos'\n",
    "frames_root_dir = 'rgb_frames'\n",
    "os.makedirs(frames_root_dir, exist_ok=True)\n",
    "subdirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    frames_subdir = os.path.join(frames_root_dir, subdir)\n",
    "    os.makedirs(frames_subdir, exist_ok=True)\n",
    "    \n",
    "    subsubdirs = [d for d in os.listdir(os.path.join(root_dir, subdir)) if os.path.isdir(os.path.join(root_dir, subdir, d))]\n",
    "    \n",
    "    for subsubdir in subsubdirs:\n",
    "        frames_subsubdir = os.path.join(frames_subdir, subsubdir)\n",
    "        os.makedirs(frames_subsubdir, exist_ok=True)\n",
    "        \n",
    "        files = os.listdir(os.path.join(root_dir, subdir, subsubdir))\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                vidcap = cv2.VideoCapture(os.path.join(root_dir, subdir, subsubdir, file))\n",
    "                success, image = vidcap.read()\n",
    "                count = 1\n",
    "                while success:\n",
    "                    # Resize the image to 224x224\n",
    "                    resize = cv2.resize(image, (224, 224))\n",
    "                    # Save the frame as a JPEG file\n",
    "                    cv2.imwrite(os.path.join(frames_subsubdir, subsubdir + \"-%s.jpg\" % str(count).zfill(6)), resize)\n",
    "                    success, image = vidcap.read()\n",
    "                    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denseflow import dense_flow\n",
    "import os\n",
    "\n",
    "root_dir = 'short videos'\n",
    "flow_root_dir = 'flow_frames'\n",
    "os.makedirs(flow_root_dir, exist_ok=True)\n",
    "subdirs = [d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "\n",
    "for subdir in subdirs:\n",
    "    flow_subdir = os.path.join(flow_root_dir, subdir)\n",
    "    os.makedirs(flow_subdir, exist_ok=True)\n",
    "    \n",
    "    subsubdirs = [d for d in os.listdir(os.path.join(root_dir, subdir)) if os.path.isdir(os.path.join(root_dir, subdir, d))]\n",
    "    \n",
    "    for subsubdir in subsubdirs:\n",
    "        flow_subsubdir = os.path.join(flow_subdir, subsubdir)\n",
    "        os.makedirs(flow_subsubdir, exist_ok=True)\n",
    "        \n",
    "        files = os.listdir(os.path.join(root_dir, subdir, subsubdir))\n",
    "        for file in files:\n",
    "            if file.endswith('.mp4'):\n",
    "                video_path = os.path.join(root_dir, subdir, subsubdir, file)\n",
    "                video_name = subsubdir\n",
    "                save_dir = flow_subsubdir\n",
    "                step = 1\n",
    "                bound = 15\n",
    "                dense_flow_args = [video_path, video_name, save_dir, step, bound]\n",
    "                dense_flow(dense_flow_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rgb_i3d = InceptionI3d(400, in_channels=3)\n",
    "base_rgb_i3d.load_state_dict(torch.load(\"./models/rgb_imagenet.pt\"))\n",
    "base_rgb_i3d.replace_logits(8)\n",
    "base_rgb_i3d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_flow_i3d = InceptionI3d(400, in_channels=2)\n",
    "base_flow_i3d.load_state_dict(torch.load(\"./models/flow_imagenet.pt\"))\n",
    "base_flow_i3d.replace_logits(8)\n",
    "base_flow_i3d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_rgb_i3d = InceptionI3d(400, in_channels=3)\n",
    "new_rgb_i3d.replace_logits(8)\n",
    "new_rgb_i3d.load_state_dict(torch.load(\"./newmodels/rbgmodels/rgb_anamoly005000.pt\", map_location=torch.device(\"cpu\")))\n",
    "new_rgb_i3d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_flow_i3d = InceptionI3d(400, in_channels=2)\n",
    "new_flow_i3d.replace_logits(8)\n",
    "new_flow_i3d.load_state_dict(torch.load(\"./newmodels/flow_anamoly005000.pt\", map_location=torch.device(\"cpu\")))\n",
    "new_flow_i3d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_frames Arson009_x264 1 96\n",
      "flow_frames Arson009_x264 1 96\n"
     ]
    }
   ],
   "source": [
    "rgb_root = \"rgb_frames\"\n",
    "flow_root = \"flow_frames\"\n",
    "vid = \"Arson009_x264\"\n",
    "start = 1\n",
    "num_frames = len(os.listdir(os.path.join(rgb_root, vid)))\n",
    "print(rgb_root, vid, start, num_frames)\n",
    "print(flow_root, vid, start, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_frames\\Arson009_x264\n",
      "torch.Size([3, 96, 224, 224])\n",
      "torch.Size([1, 3, 96, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from charades_dataset import load_rgb_frames, load_flow_frames, video_to_tensor\n",
    "print(os.path.join(rgb_root, vid))\n",
    "rgb_imgs = load_rgb_frames(rgb_root, vid, start, num_frames)\n",
    "cropTransform = transforms.Compose([videotransforms.CenterCrop(224)])\n",
    "rgb_imgs = cropTransform(rgb_imgs)\n",
    "rgb_input = video_to_tensor(rgb_imgs)\n",
    "print(rgb_input.shape)\n",
    "rgb_input = rgb_input[None] # batch size of 1\n",
    "print(rgb_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape=(_BATCH_SIZE, _CHANNELS, _SAMPLE_VIDEO_FRAMES, _IMAGE_SIZE, _IMAGE_SIZE))\n",
    "with torch.no_grad():\n",
    "    rgb_base_prediction = base_rgb_i3d(rgb_input)\n",
    "with torch.no_grad():\n",
    "    rgb_prediction = new_rgb_i3d(rgb_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_frames\\Arson009_x264\n",
      "torch.Size([2, 95, 224, 224])\n",
      "torch.Size([1, 2, 95, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from charades_dataset import load_rgb_frames, load_flow_frames, video_to_tensor\n",
    "print(os.path.join(flow_root, vid))\n",
    "flow_imgs = load_flow_frames(flow_root, vid, start, num_frames - 1)\n",
    "cropTransform = transforms.Compose([videotransforms.CenterCrop(224)])\n",
    "flow_imgs = cropTransform(flow_imgs)\n",
    "flow_input = video_to_tensor(flow_imgs)\n",
    "print(flow_input.shape)\n",
    "flow_input = flow_input[None] # batch size of 1\n",
    "print(flow_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape=(_BATCH_SIZE, _CHANNELS, _SAMPLE_VIDEO_FRAMES, _IMAGE_SIZE, _IMAGE_SIZE))\n",
    "with torch.no_grad():\n",
    "    flow_base_prediction = base_flow_i3d(flow_input)\n",
    "with torch.no_grad():\n",
    "    flow_prediction = new_flow_i3d(flow_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 11])\n",
      "torch.Size([1, 8, 11])\n",
      "torch.Size([1, 8, 11])\n",
      "torch.Size([1, 8, 11])\n"
     ]
    }
   ],
   "source": [
    "print(rgb_base_prediction.shape)\n",
    "print(rgb_prediction.shape)\n",
    "print(flow_base_prediction.shape)\n",
    "print(flow_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: Arson\n",
    "# 1: Assault\n",
    "# 2: Fighting\n",
    "# 3: RoadAccidents\n",
    "# 4: Shooting\n",
    "# 5: Stealing\n",
    "# 6: Vandalism\n",
    "# 7: Normal\n",
    "classes = [\"Arson\", \"Assault\", \"Fighting\", \"RoadAccidents\", \"Shooting\", \"Stealing\", \"Vandalism\", \"Normal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prediction = (rgb_base_prediction + flow_base_prediction)/2.0\n",
    "_prediction = (rgb_prediction + flow_prediction)/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stealing\n",
      "Arson\n"
     ]
    }
   ],
   "source": [
    "base_predicted_class = torch.max(base_prediction, dim=2)[0].argmax(1).item()\n",
    "_predicted_class = torch.max(_prediction, dim=2)[0].argmax(1).item()\n",
    "\n",
    "print(classes[base_predicted_class])\n",
    "print(classes[_predicted_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
