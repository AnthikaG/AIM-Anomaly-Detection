{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio\n",
    "import torch\n",
    "import torchvision\n",
    "# import torchaudio\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "# print(torchaudio.__version__)\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from pytorch_i3d import InceptionI3d\n",
    "import videotransforms\n",
    "\n",
    "!pip3 install av\n",
    "import av\n",
    "print(av.__version__)\n",
    "\n",
    "# !pip3 install opencv-python\n",
    "!pip3 install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "  dev = \"cuda:0\"\n",
    "else:\n",
    "  dev = \"cpu\"\n",
    "device = torch.device(dev)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python train_i3d.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./newmodels/\n",
    "!python train_i3d.py -mode rgb -save_model ./newmodels/rgb_ucf -root ./rgb_frames/ -train_split ./ucf101.json -max_steps 100 -batch_size 1 -num_classes 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./newmodels/\n",
    "!python train_i3d.py -mode flow -save_model ./newmodels/flow_ucf -root ./new_ucf101 -train_split ./ucf101.json -max_steps 3000 -batch_size 5 -num_classes 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i3d = InceptionI3d(400, in_channels=3)\n",
    "i3d.replace_logits(157)\n",
    "i3d.load_state_dict(torch.load(\"./models/rgb_charades.pt\"))\n",
    "i3d.eval()\n",
    "transform = transforms.Compose([videotransforms.CenterCrop(224)])  # Define the transformations to apply to the videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"rgb_frames\"\n",
    "vid = \"workingAtLaptop\"\n",
    "start = 1\n",
    "num_frames = len(os.listdir(os.path.join(root, vid)))\n",
    "print(root, vid, start, num_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charades_dataset import load_rgb_frames, video_to_tensor\n",
    "print(os.path.join(root, vid))\n",
    "imgs = load_rgb_frames(root, vid, start, num_frames)\n",
    "cropTransform = transforms.Compose([videotransforms.CenterCrop(224)])\n",
    "imgs = cropTransform(imgs)\n",
    "input = video_to_tensor(imgs)\n",
    "print(input.shape)\n",
    "input = input[None] # batch size of 1\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape=(_BATCH_SIZE, _CHANNELS, _SAMPLE_VIDEO_FRAMES, _IMAGE_SIZE, _IMAGE_SIZE))\n",
    "with torch.no_grad():\n",
    "    prediction = i3d(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "with open(\"classes.txt\", \"r\") as f:\n",
    "    counter = 0\n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "    for line in lines:\n",
    "        classes[counter] = line\n",
    "        counter += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction.shape)\n",
    "print(torch.max(prediction, dim=2)[0].argmax(1)) # first max to get max of all classes, then argmax to get index of max class\n",
    "predicted_class = torch.max(prediction, dim=2)[0].argmax(1).item()\n",
    "print(classes[predicted_class])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
